{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from embeddings import load_embeddings, load_vocab\n",
    "from load_conll import load_conll03\n",
    "from loader import prepare_sentence, tag_mapping, cap_feature, CoNLLDataset, pad_list\n",
    "from model_char import Tagger, cuda\n",
    "from torch_utils import prepare_sequence, prepare_sequence_float, tensor\n",
    "from utils import sent2seq, sent2chars, word_index, char_index, add_unknown_last, zero_digits\n",
    "from eval import eval, micro_precision_recall_f1_accuracy, eval_metrics, eval_metrics_crf, save_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:0.1-optimizer:sgd-hidden_size:200-pre_emb:glove-w_embed_size:300-batch_size:4-c_embed_size:25-char_hidden_size:100-load_embeds:true-dropout:0.5-gradient_clipping:0-crf:true\n"
     ]
    }
   ],
   "source": [
    "#Parameters\n",
    "torch.manual_seed(0)\n",
    "\n",
    "parameters = OrderedDict()\n",
    "\n",
    "parameters[\"lr\"] = 0.1\n",
    "parameters[\"optimizer\"] = \"SGD\"\n",
    "parameters[\"hidden_size\"] = 200\n",
    "parameters[\"pre_emb\"] = \"glove\"\n",
    "\n",
    "parameters[\"w_embed_size\"] = 300\n",
    "# parameters[\"dim_cap\"] = 10\n",
    "\n",
    "parameters[\"batch_size\"] = 4\n",
    "\n",
    "parameters[\"c_embed_size\"] = 25\n",
    "parameters[\"char_hidden_size\"] = 100\n",
    "\n",
    "parameters[\"load_embeds\"] = True\n",
    "parameters[\"dropout\"] = 0.5\n",
    "parameters[\"gradient_clipping\"] = 0\n",
    "parameters[\"crf\"] = True\n",
    "\n",
    "epochs = 1000\n",
    "zero_digit = True\n",
    "\n",
    "assert parameters[\"pre_emb\"] in [\"glove\", \"google\"]\n",
    "assert not parameters[\"pre_emb\"] == \"google\" or parameters[\"w_embed_size\"] == 300\n",
    "\n",
    "param_str = \"-\".join([\"%s:%s\" % (str(k), str(v)) for (k,v) in parameters.items()]).lower()\n",
    "print(param_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from saved embeddings\n",
      "Loading vocab\n"
     ]
    }
   ],
   "source": [
    "if parameters[\"pre_emb\"] == \"glove\":\n",
    "    embeddings_path = \"word_embeddings/glove.6B/glove.6B.%sd_w2vformat.txt\" % parameters[\"w_embed_size\"]\n",
    "    binary = False\n",
    "else:\n",
    "    embeddings_path = \"word_embeddings/google/GoogleNews-vectors-negative300.bin\"\n",
    "    binary = True\n",
    "    \n",
    "if parameters[\"load_embeds\"]:\n",
    "    loaded_embeddings, (w2idx, idx2w) = load_embeddings(embeddings_path, binary=binary)\n",
    "else:\n",
    "    parameters[\"freeze\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CoNLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CoNLL03 in 1.79341459274292 seconds\n",
      "Loaded CoNLL03 in 0.43091535568237305 seconds\n",
      "Loaded CoNLL03 in 0.4309520721435547 seconds\n",
      "Train 14041, Dev 3250, Test 3453\n"
     ]
    }
   ],
   "source": [
    "# CoNLL03\n",
    "sents_train_03, pos_train_03, chunk_train_03, ner_train_03 = load_conll03([\"cleaned_eng.train\"])\n",
    "sents_dev_03, pos_dev_03, chunk_train_03, ner_dev_03 = load_conll03([\"cleaned_eng.testa\"])\n",
    "sents_test_03, pos_test_03, chunk_train_03, ner_test_03 = load_conll03([\"cleaned_eng.testb\"])\n",
    "\n",
    "print(\"Train %s, Dev %s, Test %s\" % (len(sents_train_03), len(sents_dev_03), len(sents_test_03)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if zero_digit:\n",
    "    sents_train_03 = [[zero_digits(w) for w in s] for s in sents_train_03]\n",
    "    sents_test_03 = [[zero_digits(w) for w in s] for s in sents_test_03]    \n",
    "    sents_dev_03 = [[zero_digits(w) for w in s] for s in sents_dev_03]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents_train = np.concatenate([sents_train_03, sents_dev_03, sents_test_03])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 unique named entity tags\n"
     ]
    }
   ],
   "source": [
    "w2idx_train, idx2w_train = word_index(sents_train)\n",
    "w2idx_train, idx2w_train = add_unknown_last(w2idx_train, idx2w_train)\n",
    "\n",
    "X_train_03 = sent2seq(sents_train_03, w2idx_train)\n",
    "X_dev_03 = sent2seq(sents_dev_03, w2idx_train)\n",
    "X_test_03 = sent2seq(sents_test_03, w2idx_train)\n",
    "\n",
    "idner_train, ner2idx, idx2ner = tag_mapping(ner_train_03)\n",
    "idner_dev = tag_mapping(ner_dev_03, ner2idx)\n",
    "idner_test = tag_mapping(ner_test_03, ner2idx)\n",
    "\n",
    "num_ner_classes = len(ner2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c2idx, idx2c = char_index(sents_train)\n",
    "char_embeddings = np.random.normal(scale=0.001, size=(len(w2idx_train), parameters[\"c_embed_size\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars_train_03 = sent2chars(sents_train_03, c2idx)\n",
    "chars_dev_03 = sent2chars(sents_dev_03, c2idx)\n",
    "chars_test_03 = sent2chars(sents_test_03, c2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CoNLLDataset_chars(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, chars, y, lens):\n",
    "        self.words = X\n",
    "        self.chars = chars\n",
    "        self.labels = y\n",
    "        self.lens = lens\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.words[idx], self.chars[idx],self.labels[idx], self.lens[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_chars(chars, pad_index=0):\n",
    "    lens_sents = [len(s) for s in chars]\n",
    "    lens_words = [[len(w) for w in s] for s in chars]\n",
    "    \n",
    "    maxlen_sent = max(lens_sents)\n",
    "    maxlen = max(np.concatenate(lens_words))\n",
    "    \n",
    "    \n",
    "    unrolled = []\n",
    "    for s in chars:\n",
    "        for w in s:\n",
    "            unrolled.append(w)\n",
    "    \n",
    "    batch = pad_index * torch.ones(len(chars), int(maxlen_sent), int(maxlen)).long()\n",
    "    \n",
    "    for i, s in enumerate(chars):\n",
    "        for j, w in enumerate(s):\n",
    "            batch[i, j, :lens_words[i][j]] = torch.LongTensor(w)\n",
    "        \n",
    "    return batch, lens_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, lens_words = pad_chars(chars_dev_03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   25    14    23  ...      0     0     0\n",
      "   56    39     3  ...      0     0     0\n",
      "   28     0     0  ...      0     0     0\n",
      "       ...          ⋱          ...       \n",
      "   27    27     0  ...      0     0     0\n",
      "   48     0     0  ...      0     0     0\n",
      "   20     0     0  ...      0     0     0\n",
      "[torch.LongTensor of size 109x27]\n",
      "\n",
      "109\n",
      "[5, 6, 1, 2, 1, 5, 7, 1, 2, 1, 6, 5, 1, 2, 1, 5, 6, 1, 4, 8, 1, 2, 4, 1, 2, 1, 5, 3, 9, 1, 2, 1, 6, 8, 1, 2, 1, 5, 8, 1, 1, 1, 5, 3, 3, 10, 1, 1, 1, 4, 9, 1, 7, 1, 1, 1, 1, 5, 6, 1, 5, 5, 1, 2, 1, 1, 1, 1, 5, 5, 1, 1, 1, 4, 7, 1, 5, 3, 7, 1, 2, 1, 1, 1, 1, 5, 5, 1, 1, 1, 6, 6, 1, 1, 1, 5, 6, 1, 1, 1, 5, 6, 1, 5, 5, 1, 2, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(batch[0])\n",
    "print(len(lens_words[0]))\n",
    "print(lens_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_train, lens_train, sorted_train = pad_list(X_train_03)\n",
    "chars_train, wlens_train = pad_chars(chars_train_03)\n",
    "labels_train, _, _ = pad_list(idner_train)\n",
    "\n",
    "words_dev, lens_dev, sorted_dev = pad_list(X_dev_03)\n",
    "chars_dev, wlens_dev = pad_chars(chars_dev_03)\n",
    "labels_dev, _, _ = pad_list(idner_dev)\n",
    "\n",
    "words_test, lens_test, sorted_test = pad_list(X_test_03)\n",
    "chars_test, wlens_test = pad_chars(chars_test_03)\n",
    "labels_test, _, _ = pad_list(idner_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "( 0 ,.,.) = \n",
       "   1   2   0  ...    0   0   0\n",
       "   3   4   5  ...    0   0   0\n",
       "   9   4   3  ...    0   0   0\n",
       "     ...       ⋱       ...    \n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "\n",
       "( 1 ,.,.) = \n",
       "  21   4   7  ...    0   0   0\n",
       "  17  13  11  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "     ...       ⋱       ...    \n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "\n",
       "( 2 ,.,.) = \n",
       "  17  24   2  ...    0   0   0\n",
       "  27  27  27  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "     ...       ⋱       ...    \n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "\n",
       "( 3 ,.,.) = \n",
       "  29  19   4  ...    0   0   0\n",
       "   1  23   3  ...    0   0   0\n",
       "  31  14  10  ...    0   0   0\n",
       "     ...       ⋱       ...    \n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "[torch.LongTensor of size 4x113x61]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_train[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = CoNLLDataset_chars(words_train, chars_train, labels_train, lens_train)\n",
    "loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=parameters[\"batch_size\"], num_workers=0,\n",
    "                                           pin_memory=True)\n",
    "\n",
    "dataset_dev = CoNLLDataset_chars(words_dev, chars_dev,labels_dev, lens_dev)\n",
    "loader_dev = torch.utils.data.DataLoader(dataset_dev, batch_size=parameters[\"batch_size\"], num_workers=0,\n",
    "                                         pin_memory=True)\n",
    "\n",
    "dataset_test = CoNLLDataset_chars(words_test, chars_test, labels_test, lens_test)\n",
    "loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=parameters[\"batch_size\"], num_workers=0,\n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idner_dev = [idner_dev[i] for i in sorted_dev]\n",
    "idner_test = [idner_test[i] for i in sorted_test]\n",
    "\n",
    "sents_dev_03 = [sents_dev_03[i] for i in sorted_dev]\n",
    "sents_test_03 = [sents_test_03[i] for i in sorted_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for x, y, l in loader:\n",
    "#     packed = pack_padded_sequence(autograd.Variable(x), l.numpy(), batch_first=True)\n",
    "#     batch, lens = pad_packed_sequence(packed, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit word embeddings to vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = np.random.normal(scale=0.001, size=(len(w2idx_train), parameters[\"w_embed_size\"]))\n",
    "\n",
    "if parameters[\"load_embeds\"]:\n",
    "    for w, i in w2idx_train.items():\n",
    "        idx = w2idx.get(w)\n",
    "        if idx is not None:\n",
    "            embeddings[i] = loaded_embeddings[idx][:parameters[\"w_embed_size\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capitalization features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cap_train_03 = [[cap_feature(w) for w in s] for s in sents_train_03]\n",
    "# cap_test_03 = [[cap_feature(w) for w in s] for s in sents_test_03]\n",
    "# cap_dev_03 = [[cap_feature(w) for w in s] for s in sents_dev_03]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if parameters[\"dim_cap\"]:\n",
    "#     n_cap = 4\n",
    "#     cap_embeddings = np.random.normal(scale=0.001, size=(n_cap, parameters[\"dim_cap\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-LOC',\n",
       " 2: 'B-PER',\n",
       " 3: 'B-ORG',\n",
       " 4: 'I-PER',\n",
       " 5: 'I-ORG',\n",
       " 6: 'B-MISC',\n",
       " 7: 'I-LOC',\n",
       " 8: 'I-MISC',\n",
       " 9: '<START>',\n",
       " 10: '<STOP>'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not \"<START>\" in idx2ner.values():\n",
    "    idx2ner[len(idx2ner)] = \"<START>\"\n",
    "    idx2ner[len(idx2ner)] = \"<STOP>\"\n",
    "    ner2idx = {v:k for (k,v) in idx2ner.items()}\n",
    "\n",
    "idx2ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Tagger(tensor(embeddings),  parameters[\"hidden_size\"], idx2ner, char_embeddings=tensor(char_embeddings),\n",
    "               char_hidden_dim = parameters[\"char_hidden_size\"], dropout=parameters[\"dropout\"], \n",
    "               crf=parameters[\"crf\"])\n",
    "\n",
    "trainable_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "if parameters[\"optimizer\"].lower() == \"adam\":\n",
    "    optimizer = optim.Adam(trainable_parameters, lr= parameters[\"lr\"])\n",
    "elif parameters[\"optimizer\"].lower() == \"sgd\":\n",
    "    optimizer = optim.SGD(trainable_parameters, lr= parameters[\"lr\"])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload = 1\n",
    "model_path = \"models/ner_crf_batch/%s/\" % param_str\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    \n",
    "if reload and os.path.exists(model_path + \"last_state_dict\"):\n",
    "    model.load_state_dict(torch.load(model_path + \"last_state_dict\"))\n",
    "    model = model.cuda()\n",
    "    with open(model_path + \"metrics.p\", \"rb\") as file:\n",
    "        metrics = pickle.load(file)\n",
    "    best_ner = np.max(metrics[\"ner\"][\"val_loss_dev\"])\n",
    "    \n",
    "else:\n",
    "    metrics = {\"ner\":{\"precision\":[], \"recall\":[], \"f1\":[], \"accuracy\":[], \"ent_f1\":[], \"loss\": [], \"val_loss_dev\": [],\n",
    "                      \"precision_test\":[], \"recall_test\":[], \"f1_test\":[], \"accuracy_test\":[], \"ent_f1_test\":[], \n",
    "                      \"val_loss_test\": []}}\n",
    "    best_ner = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([4, 113])\n",
      "\n",
      "( 0 ,.,.) = \n",
      "   1   2   0  ...    0   0   0\n",
      "   3   4   5  ...    0   0   0\n",
      "   9   4   3  ...    0   0   0\n",
      "     ...       ⋱       ...    \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "\n",
      "( 1 ,.,.) = \n",
      "  21   4   7  ...    0   0   0\n",
      "  17  13  11  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "     ...       ⋱       ...    \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "\n",
      "( 2 ,.,.) = \n",
      "  17  24   2  ...    0   0   0\n",
      "  27  27  27  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "     ...       ⋱       ...    \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "\n",
      "( 3 ,.,.) = \n",
      "  29  19   4  ...    0   0   0\n",
      "   1  23   3  ...    0   0   0\n",
      "  31  14  10  ...    0   0   0\n",
      "     ...       ⋱       ...    \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "[torch.LongTensor of size 4x113x61]\n",
      "\n",
      "torch.Size([4, 113])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "for i, (words, chars, tags, lens) in enumerate(loader_train):\n",
    "    print(i)\n",
    "    print(words.size())\n",
    "    print(chars)\n",
    "    print(tags.size())\n",
    "    print(lens.size())\n",
    "    break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(\"Epoch %s/%s :\" % (epoch+1, epochs))\n",
    "\n",
    "    losses = []\n",
    "    val_loss_epoch = []\n",
    "    loss_epoch = []\n",
    "    \n",
    "    test_lim = 3500 \n",
    "    for i, (sentences, tags, lens) in enumerate(tqdm(loader_train)):\n",
    "#     test_lim = 10\n",
    "#     for i in tqdm(range(10)):\n",
    "                      \n",
    "        sentences_in = autograd.Variable(cuda(sentences[:,:lens.numpy()[0]]))\n",
    "        targets = autograd.Variable(cuda(tags[:,:lens.numpy()[0]]))\n",
    "        \n",
    "        packed_targets = pack_padded_sequence(targets, lens.numpy(), batch_first=True) \n",
    "        \n",
    "        if parameters[\"crf\"]:        \n",
    "            loss = model.neg_log_likelihood(sentences_in, lens, targets,\n",
    "                                            gradient_clipping=parameters[\"gradient_clipping\"])\n",
    "        else:\n",
    "            scores = model(sentences_in, lens)\n",
    "            padded_scores = pad_packed_sequence(scores, batch_first=True) \n",
    "            \n",
    "#             print(padded_scores[0].size())\n",
    "#             print(targets.size())\n",
    "            \n",
    "            loss = nn.CrossEntropyLoss()(padded_scores[0].contiguous().view(-1,len(ner2idx)),\n",
    "                                                          targets.contiguous().view(-1))\n",
    "\n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.cpu().data.numpy())\n",
    "    \n",
    "    preds_dev, ner_loss_dev = model.test(loader_dev)  \n",
    "    \n",
    "    eval_metrics_crf(preds_dev, metrics, idner_dev[:test_lim], sents_dev_03[:test_lim], \n",
    "                     ner2idx, idx2ner, model_path, dev=True)\n",
    "    \n",
    "    val_loss_epoch = ner_loss_dev\n",
    "    \n",
    "    preds_test, ner_loss_test = model.test(loader_test)     \n",
    "    \n",
    "    eval_metrics_crf(preds_test, metrics, idner_test[:test_lim], sents_test_03[:test_lim], \n",
    "                         ner2idx, idx2ner, model_path)\n",
    "    \n",
    "    loss_epoch = np.mean(losses)\n",
    "    \n",
    "    print(\"Loss :  NER %s\" % (loss_epoch))\n",
    "    print(\"Dev loss : NER %s\" % (val_loss_epoch))\n",
    "    print(\"Test loss : NER %s\" % (ner_loss_test))\n",
    "    \n",
    "    torch.save(model.state_dict(), model_path + \"last_state_dict\")\n",
    "    if ner_loss_dev < best_ner: \n",
    "        print(\"New best score on dev.\")\n",
    "        print(\"Saving model...\")\n",
    "        torch.save(model.state_dict(), model_path + \"best_state_dict\")\n",
    "        \n",
    "        best_ner = ner_loss_dev\n",
    "    \n",
    "    metrics[\"ner\"][\"val_loss_dev\"].append(val_loss_epoch)\n",
    "    metrics[\"ner\"][\"val_loss_test\"].append(ner_loss_test)    \n",
    "    metrics[\"ner\"][\"loss\"].append(loss_epoch)    \n",
    "\n",
    "    # Save learning curve\n",
    "    save_plot(metrics, model_path)\n",
    "    with open(model_path + \"metrics.p\", \"wb\") as file:\n",
    "        pickle.dump(metrics, file)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x,y,l in loader_train:\n",
    "    X = autograd.Variable(x)\n",
    "    Y = autograd.Variable(y)\n",
    "    L = l\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embeddings = nn.Embedding(len(w2idx_train), 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "packed = pack_padded_sequence(X, l.numpy(), batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = word_embeddings(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = pack_padded_sequence(w, l.numpy(), batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.sum(p.batch_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.sum(l.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(l.numpy())\n",
    "print(p.batch_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
