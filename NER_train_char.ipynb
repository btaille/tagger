{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from embeddings import load_embeddings, load_vocab\n",
    "from load_conll import load_conll03\n",
    "from loader import prepare_sentence, tag_mapping, cap_feature, CoNLLDataset, pad_list\n",
    "from model_char import Tagger, cuda\n",
    "from torch_utils import prepare_sequence, prepare_sequence_float, tensor\n",
    "from utils import sent2seq, sent2chars, word_index, char_index, add_unknown_last, zero_digits\n",
    "from eval import eval, micro_precision_recall_f1_accuracy, eval_metrics, eval_metrics_crf, save_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:0.001-optimizer:adam-hidden_size:300-pre_emb:glove-w_embed_size:300-batch_size:20-c_embed_size:100-char_hidden_size:100-load_embeds:true-dropout:0.5-gradient_clipping:0-crf:true\n"
     ]
    }
   ],
   "source": [
    "#Parameters\n",
    "torch.manual_seed(0)\n",
    "\n",
    "parameters = OrderedDict()\n",
    "\n",
    "parameters[\"lr\"] = 0.001\n",
    "parameters[\"optimizer\"] = \"Adam\"\n",
    "parameters[\"hidden_size\"] = 300\n",
    "parameters[\"pre_emb\"] = \"glove\"\n",
    "\n",
    "parameters[\"w_embed_size\"] = 300\n",
    "# parameters[\"dim_cap\"] = 10\n",
    "\n",
    "parameters[\"batch_size\"] = 20\n",
    "\n",
    "parameters[\"c_embed_size\"] = 100\n",
    "parameters[\"char_hidden_size\"] = 100\n",
    "\n",
    "parameters[\"load_embeds\"] = True\n",
    "parameters[\"dropout\"] = 0.5\n",
    "parameters[\"gradient_clipping\"] = 0\n",
    "parameters[\"crf\"] = True\n",
    "\n",
    "epochs = 1000\n",
    "zero_digit = True\n",
    "\n",
    "assert parameters[\"pre_emb\"] in [\"glove\", \"google\"]\n",
    "assert not parameters[\"pre_emb\"] == \"google\" or parameters[\"w_embed_size\"] == 300\n",
    "\n",
    "param_str = \"-\".join([\"%s:%s\" % (str(k), str(v)) for (k,v) in parameters.items()]).lower()\n",
    "print(param_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from saved embeddings\n",
      "Loading vocab\n"
     ]
    }
   ],
   "source": [
    "if parameters[\"pre_emb\"] == \"glove\":\n",
    "    embeddings_path = \"word_embeddings/glove.6B/glove.6B.%sd_w2vformat.txt\" % parameters[\"w_embed_size\"]\n",
    "    binary = False\n",
    "else:\n",
    "    embeddings_path = \"word_embeddings/google/GoogleNews-vectors-negative300.bin\"\n",
    "    binary = True\n",
    "    \n",
    "if parameters[\"load_embeds\"]:\n",
    "    loaded_embeddings, (w2idx, idx2w) = load_embeddings(embeddings_path, binary=binary)\n",
    "else:\n",
    "    parameters[\"freeze\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CoNLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CoNLL03 in 1.8940796852111816 seconds\n",
      "Loaded CoNLL03 in 0.4967625141143799 seconds\n",
      "Loaded CoNLL03 in 0.4694633483886719 seconds\n",
      "Train 14041, Dev 3250, Test 3453\n"
     ]
    }
   ],
   "source": [
    "# CoNLL03\n",
    "sents_train_03, pos_train_03, chunk_train_03, ner_train_03 = load_conll03([\"cleaned_eng.train\"])\n",
    "sents_dev_03, pos_dev_03, chunk_train_03, ner_dev_03 = load_conll03([\"cleaned_eng.testa\"])\n",
    "sents_test_03, pos_test_03, chunk_train_03, ner_test_03 = load_conll03([\"cleaned_eng.testb\"])\n",
    "\n",
    "print(\"Train %s, Dev %s, Test %s\" % (len(sents_train_03), len(sents_dev_03), len(sents_test_03)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if zero_digit:\n",
    "    sents_train_03 = [[zero_digits(w) for w in s] for s in sents_train_03]\n",
    "    sents_test_03 = [[zero_digits(w) for w in s] for s in sents_test_03]    \n",
    "    sents_dev_03 = [[zero_digits(w) for w in s] for s in sents_dev_03]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents_train = np.concatenate([sents_train_03, sents_dev_03, sents_test_03])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 unique named entity tags\n"
     ]
    }
   ],
   "source": [
    "w2idx_train, idx2w_train = word_index(sents_train)\n",
    "w2idx_train, idx2w_train = add_unknown_last(w2idx_train, idx2w_train)\n",
    "\n",
    "X_train_03 = sent2seq(sents_train_03, w2idx_train)\n",
    "X_dev_03 = sent2seq(sents_dev_03, w2idx_train)\n",
    "X_test_03 = sent2seq(sents_test_03, w2idx_train)\n",
    "\n",
    "idner_train, ner2idx, idx2ner = tag_mapping(ner_train_03)\n",
    "idner_dev = tag_mapping(ner_dev_03, ner2idx)\n",
    "idner_test = tag_mapping(ner_test_03, ner2idx)\n",
    "\n",
    "num_ner_classes = len(ner2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c2idx, idx2c = char_index(sents_train)\n",
    "c2idx, idx2c = add_unknown_last(c2idx, idx2c)\n",
    "char_embeddings = np.random.normal(scale=0.001, size=(len(c2idx), parameters[\"c_embed_size\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars_train_03 = sent2chars(sents_train_03, c2idx)\n",
    "chars_dev_03 = sent2chars(sents_dev_03, c2idx)\n",
    "chars_test_03 = sent2chars(sents_test_03, c2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CoNLLDataset_chars(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, chars, y, lens, wlens, wsorted):\n",
    "        self.words = X\n",
    "        self.chars = chars\n",
    "        self.labels = y\n",
    "        self.lens = lens\n",
    "        self.wlens = wlens\n",
    "        self.wsorted = wsorted\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.words[idx], self.chars[idx], self.labels[idx], self.lens[idx], self.wlens[idx], self.wsorted[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_chars(chars, pad_index=0):\n",
    "    lens_sents = [len(s) for s in chars]\n",
    "    lens_words = [[len(w) for w in s] for s in chars]\n",
    "    sorted_sents = sorted(range(len(lens_sents)), key=lambda k: lens_sents[k], reverse=True)\n",
    "        \n",
    "    maxlen_sent = max(lens_sents)\n",
    "    maxlen = max(np.concatenate(lens_words))\n",
    "    \n",
    "    \n",
    "    unrolled = []\n",
    "    for s in chars:\n",
    "        for w in s:\n",
    "            unrolled.append(w)\n",
    "    \n",
    "    batch = pad_index * torch.ones(len(chars), int(maxlen_sent), int(maxlen)).long()\n",
    "    sorted_indices = pad_index * torch.ones(len(chars), int(maxlen_sent)).long()\n",
    "    wlens = pad_index * torch.ones(len(chars), int(maxlen_sent)).long()\n",
    "    \n",
    "    for i, s in enumerate(sorted_sents):\n",
    "        ordered, _, sorted_ids = pad_list(chars[s], pad_index)\n",
    "        for j, w in enumerate(ordered):\n",
    "            batch[i, j, :lens_words[s][sorted_ids[j]]] = torch.LongTensor(w[:lens_words[s][sorted_ids[j]]])\n",
    "            sorted_indices[i, :lens_sents[s]] = torch.LongTensor(sorted_ids)\n",
    "            wlens[i, :lens_sents[s]] = torch.LongTensor(lens_words[s])\n",
    "            \n",
    "    return batch, wlens, sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_train, lens_train, sorted_train = pad_list(X_train_03)\n",
    "chars_train, wlens_train, wsorted_train = pad_chars(chars_train_03)\n",
    "labels_train, _, _ = pad_list(idner_train)\n",
    "\n",
    "words_dev, lens_dev, sorted_dev = pad_list(X_dev_03)\n",
    "chars_dev, wlens_dev, wsorted_dev = pad_chars(chars_dev_03)\n",
    "labels_dev, _, _ = pad_list(idner_dev)\n",
    "\n",
    "words_test, lens_test, sorted_test = pad_list(X_test_03)\n",
    "chars_test, wlens_test, wsorted_test = pad_chars(chars_test_03)\n",
    "labels_test, _, _ = pad_list(idner_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_train = CoNLLDataset_chars(words_train, chars_train, labels_train, lens_train, wlens_train, wsorted_train)\n",
    "loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=parameters[\"batch_size\"], num_workers=0,\n",
    "                                           pin_memory=True)\n",
    "\n",
    "dataset_dev = CoNLLDataset_chars(words_dev, chars_dev,labels_dev, lens_dev, wlens_dev, wsorted_dev)\n",
    "loader_dev = torch.utils.data.DataLoader(dataset_dev, batch_size=parameters[\"batch_size\"], num_workers=0,\n",
    "                                         pin_memory=True)\n",
    "\n",
    "dataset_test = CoNLLDataset_chars(words_test, chars_test, labels_test, lens_test, wlens_test, wsorted_test)\n",
    "loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=parameters[\"batch_size\"], num_workers=0,\n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idner_dev = [idner_dev[i] for i in sorted_dev]\n",
    "idner_test = [idner_test[i] for i in sorted_test]\n",
    "\n",
    "sents_dev_03 = [sents_dev_03[i] for i in sorted_dev]\n",
    "sents_test_03 = [sents_test_03[i] for i in sorted_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit word embeddings to vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = np.random.normal(scale=0.001, size=(len(w2idx_train), parameters[\"w_embed_size\"]))\n",
    "\n",
    "if parameters[\"load_embeds\"]:\n",
    "    for w, i in w2idx_train.items():\n",
    "        idx = w2idx.get(w)\n",
    "        if idx is not None:\n",
    "            embeddings[i] = loaded_embeddings[idx][:parameters[\"w_embed_size\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capitalization features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cap_train_03 = [[cap_feature(w) for w in s] for s in sents_train_03]\n",
    "# cap_test_03 = [[cap_feature(w) for w in s] for s in sents_test_03]\n",
    "# cap_dev_03 = [[cap_feature(w) for w in s] for s in sents_dev_03]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if parameters[\"dim_cap\"]:\n",
    "#     n_cap = 4\n",
    "#     cap_embeddings = np.random.normal(scale=0.001, size=(n_cap, parameters[\"dim_cap\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-LOC',\n",
       " 2: 'B-PER',\n",
       " 3: 'B-ORG',\n",
       " 4: 'I-PER',\n",
       " 5: 'I-ORG',\n",
       " 6: 'B-MISC',\n",
       " 7: 'I-LOC',\n",
       " 8: 'I-MISC',\n",
       " 9: '<START>',\n",
       " 10: '<STOP>'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not \"<START>\" in idx2ner.values():\n",
    "    idx2ner[len(idx2ner)] = \"<START>\"\n",
    "    idx2ner[len(idx2ner)] = \"<STOP>\"\n",
    "    ner2idx = {v:k for (k,v) in idx2ner.items()}\n",
    "\n",
    "idx2ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Tagger(tensor(embeddings),  parameters[\"hidden_size\"], idx2ner, char_embeddings=tensor(char_embeddings),\n",
    "               char_hidden_dim = parameters[\"char_hidden_size\"], dropout=parameters[\"dropout\"], \n",
    "               crf=parameters[\"crf\"])\n",
    "\n",
    "trainable_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "if parameters[\"optimizer\"].lower() == \"adam\":\n",
    "    optimizer = optim.Adam(trainable_parameters, lr= parameters[\"lr\"])\n",
    "elif parameters[\"optimizer\"].lower() == \"sgd\":\n",
    "    optimizer = optim.SGD(trainable_parameters, lr= parameters[\"lr\"])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload = 0\n",
    "model_path = \"models/ner/%s/\" % param_str\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    \n",
    "if reload and os.path.exists(model_path + \"last_state_dict\"):\n",
    "    model.load_state_dict(torch.load(model_path + \"last_state_dict\"))\n",
    "    model = model.cuda()\n",
    "    with open(model_path + \"metrics.p\", \"rb\") as file:\n",
    "        metrics = pickle.load(file)\n",
    "    best_ner = np.max(metrics[\"ner\"][\"val_loss_dev\"])\n",
    "    \n",
    "else:\n",
    "    metrics = {\"ner\":{\"precision\":[], \"recall\":[], \"f1\":[], \"accuracy\":[], \"ent_f1\":[], \"loss\": [], \"val_loss_dev\": [],\n",
    "                      \"precision_test\":[], \"recall_test\":[], \"f1_test\":[], \"accuracy_test\":[], \"ent_f1_test\":[], \n",
    "                      \"val_loss_test\": []}}\n",
    "    best_ner = np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/703 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 703/703 [01:37<00:00,  7.21it/s]\n",
      "163it [00:36, 11.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner : p 0.940446892457, r 0.943801944488, f 0.942121431506, acc 0.9374440247653908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID     NE  Total      O  B-LOC  B-PER  B-ORG  I-PER  I-ORG B-MISC  I-LOC I-MISC<START> <STOP>  Percent\n",
      " 0      O  42759  42654     37     11     21      3     11     17      0      5      0      0   99.754\n",
      " 1  B-LOC   1837    163   1625      7     26      2      0     10      2      2      0      0   88.459\n",
      " 2  B-PER   1842    455     61   1302     12      9      1      2      0      0      0      0   70.684\n",
      " 3  B-ORG   1341    377     86      8    856      1      4      9      0      0      0      0   63.833\n",
      " 4  I-PER   1307    598     21     37      1    647      2      0      0      1      0      0   49.503\n",
      " 5  I-ORG    751    343     24      3     79      2    288      1      8      3      0      0   38.349\n",
      " 6 B-MISC    922    226     45      5     13      0      0    629      0      4      0      0   68.221\n",
      " 7  I-LOC    257     43     34      0      0      2     25      0    148      5      0      0   57.588\n",
      " 8 I-MISC    346    179      3      0      0      0      2     16      2    144      0      0   41.618\n",
      " 9<START>      0      0      0      0      0      0      0      0      0      0      0      0    0.000\n",
      "10 <STOP>      0      0      0      0      0      0      0      0      0      0      0      0    0.000\n",
      "48293/51362 (94.02477%)\n",
      "NER f1 : 69.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "173it [00:34, 11.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner : p 0.923208191126, r 0.924706289621, f 0.923956633122, acc 0.9204048670184128\n",
      "ID     NE  Total      O  B-LOC  B-PER  B-ORG  I-PER  I-ORG B-MISC  I-LOC I-MISC<START> <STOP>  Percent\n",
      " 0      O  38323  38091     54     31     39      7     23     43      3     32      0      0   99.395\n",
      " 1  B-LOC   1668    157   1423      8     62      2      3     11      1      1      0      0   85.312\n",
      " 2  B-PER   1617    587     43    946     35      5      0      0      0      1      0      0   58.503\n",
      " 3  B-ORG   1661    458    111     27   1021      5     12     26      0      1      0      0   61.469\n",
      " 4  I-PER   1156    645     26     32      5    445      1      1      1      0      0      0   38.495\n",
      " 5  I-ORG    835    307     28      3    171      8    286      4      8     20      0      0   34.251\n",
      " 6 B-MISC    702    253     29      2     13      0      0    400      0      5      0      0   56.980\n",
      " 7  I-LOC    257     53     24      0      3      1     48      0    127      1      0      0   49.416\n",
      " 8 I-MISC    216    121      1      0      1      0      2     11      0     80      0      0   37.037\n",
      " 9<START>      0      0      0      0      0      0      0      0      0      0      0      0    0.000\n",
      "10 <STOP>      0      0      0      0      0      0      0      0      0      0      0      0    0.000\n",
      "42819/46435 (92.21277%)\n",
      "NER f1 : 59.63\n",
      "Loss :  NER 0.82459\n",
      "Dev loss : NER 2.48705\n",
      "Test loss : NER 3.08378\n",
      "New best score on dev.\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/703 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 703/703 [01:39<00:00,  7.09it/s]\n",
      "163it [00:35, 11.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner : p 0.944445529662, r 0.947722283205, f 0.946081069182, acc 0.941337954129512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID     NE  Total      O  B-LOC  B-PER  B-ORG  I-PER  I-ORG B-MISC  I-LOC I-MISC<START> <STOP>  Percent\n",
      " 0      O  42759  42698     14     10      6      3     10     10      2      6      0      0   99.857\n",
      " 1  B-LOC   1837    187   1603     12     21      0      1     10      1      2      0      0   87.262\n",
      " 2  B-PER   1842    436     18   1368      8      8      1      3      0      0      0      0   74.267\n",
      " 3  B-ORG   1341    471     44     26    776      2     11     11      0      0      0      0   57.867\n",
      " 4  I-PER   1307    566      4     24      1    709      2      0      1      0      0      0   54.246\n",
      " 5  I-ORG    751    335     16      2     33      1    354      0      6      4      0      0   47.137\n",
      " 6 B-MISC    922    225     20      9      4      0      0    661      0      3      0      0   71.692\n",
      " 7  I-LOC    257     43     12      0      0      0     19      0    180      3      0      0   70.039\n",
      " 8 I-MISC    346    180      3      0      0      0      2     10      0    151      0      0   43.642\n",
      " 9<START>      0      0      0      0      0      0      0      0      0      0      0      0    0.000\n",
      "10 <STOP>      0      0      0      0      0      0      0      0      0      0      0      0    0.000\n",
      "48500/51362 (94.42779%)\n",
      "NER f1 : 72.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "173it [00:34, 11.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner : p 0.927471435668, r 0.929076786603, f 0.928273417065, acc 0.9247550339183805\n",
      "ID     NE  Total      O  B-LOC  B-PER  B-ORG  I-PER  I-ORG B-MISC  I-LOC I-MISC<START> <STOP>  Percent\n",
      " 0      O  38323  38126     27     35     12      8     35     43      4     33      0      0   99.486\n",
      " 1  B-LOC   1668    176   1394     17     57      2      4     15      2      1      0      0   83.573\n",
      " 2  B-PER   1617    580     20    990     17      6      3      1      0      0      0      0   61.224\n",
      " 3  B-ORG   1661    523     79     40    987      1     10     21      0      0      0      0   59.422\n",
      " 4  I-PER   1156    643      4     19      2    482      4      0      2      0      0      0   41.696\n",
      " 5  I-ORG    835    305     12      3     86     10    399      3      7     10      0      0   47.784\n",
      " 6 B-MISC    702    255     16      8      7      0      0    413      0      3      0      0   58.832\n",
      " 7  I-LOC    257     54     11      0      1      3     32      0    150      6      0      0   58.366\n",
      " 8 I-MISC    216    123      0      0      0      0      2      7      1     83      0      0   38.426\n",
      " 9<START>      0      0      0      0      0      0      0      0      0      0      0      0    0.000\n",
      "10 <STOP>      0      0      0      0      0      0      0      0      0      0      0      0    0.000\n",
      "43024/46435 (92.65425%)\n",
      "NER f1 : 63.84\n",
      "Loss :  NER 0.431629\n",
      "Dev loss : NER 2.38515\n",
      "Test loss : NER 3.07565\n",
      "New best score on dev.\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/703 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 79/703 [00:22<02:25,  4.28it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(\"Epoch %s/%s :\" % (epoch+1, epochs))\n",
    "\n",
    "    losses = []\n",
    "    val_loss_epoch = []\n",
    "    loss_epoch = []\n",
    "    \n",
    "    test_lim = 3500 \n",
    "    for i, (words, chars, tags, lens, wlens, wsorted) in enumerate(tqdm(loader_train)):\n",
    "#     test_lim = 10\n",
    "#     for i in tqdm(range(10)):\n",
    "                      \n",
    "        words_in = autograd.Variable(cuda(words[:,:lens.numpy()[0]]))\n",
    "        chars_in = autograd.Variable(cuda(chars[:,:lens.numpy()[0]]))\n",
    "        targets = autograd.Variable(cuda(tags[:,:lens.numpy()[0]]))\n",
    "        \n",
    "#         packed_targets = pack_padded_sequence(targets, lens.numpy(), batch_first=True) \n",
    "        \n",
    "        if parameters[\"crf\"]:        \n",
    "            loss = model.neg_log_likelihood(words_in, lens, targets, chars=chars_in, wlens=wlens, wsorted=wsorted,\n",
    "                                            gradient_clipping=parameters[\"gradient_clipping\"])\n",
    "        else:\n",
    "            scores = model(sentences_in, lens)\n",
    "            padded_scores = pad_packed_sequence(scores, batch_first=True) \n",
    "            \n",
    "#             print(padded_scores[0].size())\n",
    "#             print(targets.size())\n",
    "            \n",
    "            loss = nn.CrossEntropyLoss()(padded_scores[0].contiguous().view(-1,len(ner2idx)),\n",
    "                                                          targets.contiguous().view(-1))\n",
    "\n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.cpu().data.numpy())\n",
    "    \n",
    "    preds_dev, ner_loss_dev = model.test(loader_dev)  \n",
    "    \n",
    "    eval_metrics_crf(preds_dev, metrics, idner_dev[:test_lim], sents_dev_03[:test_lim], \n",
    "                     ner2idx, idx2ner, model_path, dev=True)\n",
    "    \n",
    "    val_loss_epoch = ner_loss_dev\n",
    "    \n",
    "    preds_test, ner_loss_test = model.test(loader_test)     \n",
    "    \n",
    "    eval_metrics_crf(preds_test, metrics, idner_test[:test_lim], sents_test_03[:test_lim], \n",
    "                         ner2idx, idx2ner, model_path)\n",
    "    \n",
    "    loss_epoch = np.mean(losses)\n",
    "    \n",
    "    print(\"Loss :  NER %s\" % (loss_epoch))\n",
    "    print(\"Dev loss : NER %s\" % (val_loss_epoch))\n",
    "    print(\"Test loss : NER %s\" % (ner_loss_test))\n",
    "    \n",
    "    torch.save(model.state_dict(), model_path + \"last_state_dict\")\n",
    "    if ner_loss_dev < best_ner: \n",
    "        print(\"New best score on dev.\")\n",
    "        print(\"Saving model...\")\n",
    "        torch.save(model.state_dict(), model_path + \"best_state_dict\")\n",
    "        \n",
    "        best_ner = ner_loss_dev\n",
    "    \n",
    "    metrics[\"ner\"][\"val_loss_dev\"].append(val_loss_epoch)\n",
    "    metrics[\"ner\"][\"val_loss_test\"].append(ner_loss_test)    \n",
    "    metrics[\"ner\"][\"loss\"].append(loss_epoch)    \n",
    "\n",
    "    # Save learning curve\n",
    "    save_plot(metrics, model_path)\n",
    "    with open(model_path + \"metrics.p\", \"wb\") as file:\n",
    "        pickle.dump(metrics, file)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
