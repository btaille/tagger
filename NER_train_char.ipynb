{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from embeddings import load_embeddings, load_vocab\n",
    "from load_conll import load_conll03\n",
    "from loader import prepare_sentence, tag_mapping, cap_feature, CoNLLDataset, pad_list\n",
    "from model_char import Tagger, cuda\n",
    "from torch_utils import prepare_sequence, prepare_sequence_float, tensor\n",
    "from utils import sent2seq, sent2chars, word_index, char_index, add_unknown_last, zero_digits\n",
    "from eval import eval, micro_precision_recall_f1_accuracy, eval_metrics, eval_metrics_crf, save_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:0.01-optimizer:sgd-hidden_size:200-pre_emb:glove-w_embed_size:300-batch_size:1-c_embed_size:100-char_hidden_size:100-load_embeds:true-dropout:0.5-gradient_clipping:5-crf:true\n"
     ]
    }
   ],
   "source": [
    "#Parameters\n",
    "torch.manual_seed(0)\n",
    "\n",
    "parameters = OrderedDict()\n",
    "\n",
    "parameters[\"lr\"] = 0.01\n",
    "parameters[\"optimizer\"] = \"SGD\"\n",
    "parameters[\"hidden_size\"] = 200\n",
    "parameters[\"pre_emb\"] = \"glove\"\n",
    "\n",
    "parameters[\"w_embed_size\"] = 300\n",
    "# parameters[\"dim_cap\"] = 10\n",
    "\n",
    "parameters[\"batch_size\"] = 20\n",
    "\n",
    "parameters[\"c_embed_size\"] = 100\n",
    "parameters[\"char_hidden_size\"] = 100\n",
    "\n",
    "parameters[\"load_embeds\"] = True\n",
    "parameters[\"dropout\"] = 0.5\n",
    "parameters[\"gradient_clipping\"] = 5\n",
    "parameters[\"crf\"] = True\n",
    "\n",
    "epochs = 1000\n",
    "zero_digit = True\n",
    "\n",
    "assert parameters[\"pre_emb\"] in [\"glove\", \"google\"]\n",
    "assert not parameters[\"pre_emb\"] == \"google\" or parameters[\"w_embed_size\"] == 300\n",
    "\n",
    "param_str = \"-\".join([\"%s:%s\" % (str(k), str(v)) for (k,v) in parameters.items()]).lower()\n",
    "print(param_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from saved embeddings\n",
      "Loading vocab\n"
     ]
    }
   ],
   "source": [
    "if parameters[\"pre_emb\"] == \"glove\":\n",
    "    embeddings_path = \"word_embeddings/glove.6B/glove.6B.%sd_w2vformat.txt\" % parameters[\"w_embed_size\"]\n",
    "    binary = False\n",
    "else:\n",
    "    embeddings_path = \"word_embeddings/google/GoogleNews-vectors-negative300.bin\"\n",
    "    binary = True\n",
    "    \n",
    "if parameters[\"load_embeds\"]:\n",
    "    loaded_embeddings, (w2idx, idx2w) = load_embeddings(embeddings_path, binary=binary)\n",
    "else:\n",
    "    parameters[\"freeze\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CoNLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CoNLL03 in 2.0480763912200928 seconds\n",
      "Loaded CoNLL03 in 0.5246250629425049 seconds\n",
      "Loaded CoNLL03 in 0.47431516647338867 seconds\n",
      "Train 14041, Dev 3250, Test 3453\n"
     ]
    }
   ],
   "source": [
    "# CoNLL03\n",
    "sents_train_03, pos_train_03, chunk_train_03, ner_train_03 = load_conll03([\"cleaned_eng.train\"])\n",
    "sents_dev_03, pos_dev_03, chunk_train_03, ner_dev_03 = load_conll03([\"cleaned_eng.testa\"])\n",
    "sents_test_03, pos_test_03, chunk_train_03, ner_test_03 = load_conll03([\"cleaned_eng.testb\"])\n",
    "\n",
    "print(\"Train %s, Dev %s, Test %s\" % (len(sents_train_03), len(sents_dev_03), len(sents_test_03)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if zero_digit:\n",
    "    sents_train_03 = [[zero_digits(w) for w in s] for s in sents_train_03]\n",
    "    sents_test_03 = [[zero_digits(w) for w in s] for s in sents_test_03]    \n",
    "    sents_dev_03 = [[zero_digits(w) for w in s] for s in sents_dev_03]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents_train = np.concatenate([sents_train_03, sents_dev_03, sents_test_03])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 unique named entity tags\n"
     ]
    }
   ],
   "source": [
    "w2idx_train, idx2w_train = word_index(sents_train)\n",
    "w2idx_train, idx2w_train = add_unknown_last(w2idx_train, idx2w_train)\n",
    "\n",
    "X_train_03 = sent2seq(sents_train_03, w2idx_train)\n",
    "X_dev_03 = sent2seq(sents_dev_03, w2idx_train)\n",
    "X_test_03 = sent2seq(sents_test_03, w2idx_train)\n",
    "\n",
    "idner_train, ner2idx, idx2ner = tag_mapping(ner_train_03)\n",
    "idner_dev = tag_mapping(ner_dev_03, ner2idx)\n",
    "idner_test = tag_mapping(ner_test_03, ner2idx)\n",
    "\n",
    "num_ner_classes = len(ner2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c2idx, idx2c = char_index(sents_train)\n",
    "char_embeddings = np.random.normal(scale=0.001, size=(len(c2idx), parameters[\"c_embed_size\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars_train_03 = sent2chars(sents_train_03, c2idx)\n",
    "chars_dev_03 = sent2chars(sents_dev_03, c2idx)\n",
    "chars_test_03 = sent2chars(sents_test_03, c2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoNLLDataset_chars(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, chars, y, lens, wlens, wsorted):\n",
    "        self.words = X\n",
    "        self.chars = chars\n",
    "        self.labels = y\n",
    "        self.lens = lens\n",
    "        self.wlens = wlens\n",
    "        self.wsorted = wsorted\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.words[idx], self.chars[idx], self.labels[idx], self.lens[idx], self.wlens[idx], self.wsorted[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_chars(chars, pad_index=10000):\n",
    "    lens_sents = [len(s) for s in chars]\n",
    "    lens_words = [[len(w) for w in s] for s in chars]\n",
    "    sorted_sents = sorted(range(len(lens_sents)), key=lambda k: lens_sents[k], reverse=True)\n",
    "        \n",
    "    maxlen_sent = max(lens_sents)\n",
    "    maxlen = max(np.concatenate(lens_words))\n",
    "    \n",
    "    \n",
    "    unrolled = []\n",
    "    for s in chars:\n",
    "        for w in s:\n",
    "            unrolled.append(w)\n",
    "    \n",
    "    batch = pad_index * torch.ones(len(chars), int(maxlen_sent), int(maxlen)).long()\n",
    "    sorted_indices = pad_index * torch.ones(len(chars), int(maxlen_sent)).long()\n",
    "    wlens = pad_index * torch.ones(len(chars), int(maxlen_sent)).long()\n",
    "    \n",
    "    for i, s in enumerate(sorted_sents):\n",
    "        ordered, _, sorted_ids = pad_list(chars[s], pad_index)\n",
    "        for j, w in enumerate(ordered):\n",
    "            batch[i, j, :lens_words[s][sorted_ids[j]]] = torch.LongTensor(w[:lens_words[s][sorted_ids[j]]])\n",
    "            sorted_indices[i, :lens_sents[s]] = torch.LongTensor(sorted_ids)\n",
    "            wlens[i, :lens_sents[s]] = torch.LongTensor(lens_words[s])\n",
    "            \n",
    "    return batch, wlens, sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_train, lens_train, sorted_train = pad_list(X_train_03)\n",
    "chars_train, wlens_train, wsorted_train = pad_chars(chars_train_03)\n",
    "labels_train, _, _ = pad_list(idner_train)\n",
    "\n",
    "words_dev, lens_dev, sorted_dev = pad_list(X_dev_03)\n",
    "chars_dev, wlens_dev, wsorted_dev = pad_chars(chars_dev_03)\n",
    "labels_dev, _, _ = pad_list(idner_dev)\n",
    "\n",
    "words_test, lens_test, sorted_test = pad_list(X_test_03)\n",
    "chars_test, wlens_test, wsorted_test = pad_chars(chars_test_03)\n",
    "labels_test, _, _ = pad_list(idner_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_train = CoNLLDataset_chars(words_train, chars_train, labels_train, lens_train, wlens_train, wsorted_train)\n",
    "loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=parameters[\"batch_size\"], num_workers=0,\n",
    "                                           pin_memory=True)\n",
    "\n",
    "dataset_dev = CoNLLDataset_chars(words_dev, chars_dev,labels_dev, lens_dev, wlens_dev, wsorted_dev)\n",
    "loader_dev = torch.utils.data.DataLoader(dataset_dev, batch_size=parameters[\"batch_size\"], num_workers=0,\n",
    "                                         pin_memory=True)\n",
    "\n",
    "dataset_test = CoNLLDataset_chars(words_test, chars_test, labels_test, lens_test, wlens_test, wsorted_test)\n",
    "loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=parameters[\"batch_size\"], num_workers=0,\n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idner_dev = [idner_dev[i] for i in sorted_dev]\n",
    "idner_test = [idner_test[i] for i in sorted_test]\n",
    "\n",
    "sents_dev_03 = [sents_dev_03[i] for i in sorted_dev]\n",
    "sents_test_03 = [sents_test_03[i] for i in sorted_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit word embeddings to vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = np.random.normal(scale=0.001, size=(len(w2idx_train), parameters[\"w_embed_size\"]))\n",
    "\n",
    "if parameters[\"load_embeds\"]:\n",
    "    for w, i in w2idx_train.items():\n",
    "        idx = w2idx.get(w)\n",
    "        if idx is not None:\n",
    "            embeddings[i] = loaded_embeddings[idx][:parameters[\"w_embed_size\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capitalization features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cap_train_03 = [[cap_feature(w) for w in s] for s in sents_train_03]\n",
    "# cap_test_03 = [[cap_feature(w) for w in s] for s in sents_test_03]\n",
    "# cap_dev_03 = [[cap_feature(w) for w in s] for s in sents_dev_03]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if parameters[\"dim_cap\"]:\n",
    "#     n_cap = 4\n",
    "#     cap_embeddings = np.random.normal(scale=0.001, size=(n_cap, parameters[\"dim_cap\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-LOC',\n",
       " 2: 'B-PER',\n",
       " 3: 'B-ORG',\n",
       " 4: 'I-PER',\n",
       " 5: 'I-ORG',\n",
       " 6: 'B-MISC',\n",
       " 7: 'I-LOC',\n",
       " 8: 'I-MISC',\n",
       " 9: '<START>',\n",
       " 10: '<STOP>'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not \"<START>\" in idx2ner.values():\n",
    "    idx2ner[len(idx2ner)] = \"<START>\"\n",
    "    idx2ner[len(idx2ner)] = \"<STOP>\"\n",
    "    ner2idx = {v:k for (k,v) in idx2ner.items()}\n",
    "\n",
    "idx2ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Tagger(tensor(embeddings),  parameters[\"hidden_size\"], idx2ner, char_embeddings=tensor(char_embeddings),\n",
    "               char_hidden_dim = parameters[\"char_hidden_size\"], dropout=parameters[\"dropout\"], \n",
    "               crf=parameters[\"crf\"])\n",
    "\n",
    "trainable_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "if parameters[\"optimizer\"].lower() == \"adam\":\n",
    "    optimizer = optim.Adam(trainable_parameters, lr= parameters[\"lr\"])\n",
    "elif parameters[\"optimizer\"].lower() == \"sgd\":\n",
    "    optimizer = optim.SGD(trainable_parameters, lr= parameters[\"lr\"])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload = 1\n",
    "model_path = \"models/ner_crf_batch/%s/\" % param_str\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    \n",
    "if reload and os.path.exists(model_path + \"last_state_dict\"):\n",
    "    model.load_state_dict(torch.load(model_path + \"last_state_dict\"))\n",
    "    model = model.cuda()\n",
    "    with open(model_path + \"metrics.p\", \"rb\") as file:\n",
    "        metrics = pickle.load(file)\n",
    "    best_ner = np.max(metrics[\"ner\"][\"val_loss_dev\"])\n",
    "    \n",
    "else:\n",
    "    metrics = {\"ner\":{\"precision\":[], \"recall\":[], \"f1\":[], \"accuracy\":[], \"ent_f1\":[], \"loss\": [], \"val_loss_dev\": [],\n",
    "                      \"precision_test\":[], \"recall_test\":[], \"f1_test\":[], \"accuracy_test\":[], \"ent_f1_test\":[], \n",
    "                      \"val_loss_test\": []}}\n",
    "    best_ner = np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/14041 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14041/14041 [11:36<00:00, 20.17it/s]\n",
      "3250it [04:01, 13.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner : p 0.91294962617, r 0.911929590717, f 0.912439323364, acc 0.9057863790350843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID     NE  Total      O  B-LOC  B-PER  B-ORG  I-PER  I-ORG B-MISC  I-LOC I-MISC<START> <STOP>  Percent\n",
      " 0      O  42759  42226     96      3    297     46      5     40      7     39      0      0   98.753\n",
      " 1  B-LOC   1837     59   1446      2    217      6      0    102      2      3      0      0   78.715\n",
      " 2  B-PER   1842     92    554    570    574     32      0     15      3      2      0      0   30.945\n",
      " 3  B-ORG   1341    111    267      9    837      8      1    101      0      7      0      0   62.416\n",
      " 4  I-PER   1307     55    102      0     25   1037      0      1     77     10      0      0   79.342\n",
      " 5  I-ORG    751    131     59      0    151    139     44     12     88    127      0      0    5.859\n",
      " 6 B-MISC    922    124    191      3    299     10      0    288      0      7      0      0   31.236\n",
      " 7  I-LOC    257     16     40      0     20     49      0      0     75     57      0      0   29.183\n",
      " 8 I-MISC    346     72      9      0     38     40      9     16     11    151      0      0   43.642\n",
      " 9<START>      0      0      0      0      0      0      0      0      0      0      0      0    0.000\n",
      "10 <STOP>      0      0      0      0      0      0      0      0      0      0      0      0    0.000\n",
      "46674/51362 (90.87263%)\n",
      "NER f1 : 41.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1991it [01:02, 63.26it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-6ef679913c49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mval_loss_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mner_loss_dev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mpreds_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mner_loss_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     eval_metrics_crf(preds_test, metrics, idner_test[:test_lim], sents_test_03[:test_lim], \n",
      "\u001b[0;32m/notebooks/ner/model_char.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwlens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwsorted\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                 \u001b[0mwords_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mlens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m                 \u001b[0mchars_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mlens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(\"Epoch %s/%s :\" % (epoch+1, epochs))\n",
    "\n",
    "    losses = []\n",
    "    val_loss_epoch = []\n",
    "    loss_epoch = []\n",
    "    \n",
    "    test_lim = 3500 \n",
    "    for i, (words, chars, tags, lens, wlens, wsorted) in enumerate(tqdm(loader_train)):\n",
    "#     test_lim = 10\n",
    "#     for i in tqdm(range(10)):\n",
    "                      \n",
    "        words_in = autograd.Variable(cuda(words[:,:lens.numpy()[0]]))\n",
    "        chars_in = autograd.Variable(cuda(chars[:,:lens.numpy()[0]]))\n",
    "        targets = autograd.Variable(cuda(tags[:,:lens.numpy()[0]]))\n",
    "        \n",
    "        packed_targets = pack_padded_sequence(targets, lens.numpy(), batch_first=True) \n",
    "        \n",
    "        if parameters[\"crf\"]:        \n",
    "            loss = model.neg_log_likelihood(words_in, lens, targets, chars=chars_in, wlens=wlens, wsorted=wsorted,\n",
    "                                            gradient_clipping=parameters[\"gradient_clipping\"])\n",
    "        else:\n",
    "            scores = model(sentences_in, lens)\n",
    "            padded_scores = pad_packed_sequence(scores, batch_first=True) \n",
    "            \n",
    "#             print(padded_scores[0].size())\n",
    "#             print(targets.size())\n",
    "            \n",
    "            loss = nn.CrossEntropyLoss()(padded_scores[0].contiguous().view(-1,len(ner2idx)),\n",
    "                                                          targets.contiguous().view(-1))\n",
    "\n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.cpu().data.numpy())\n",
    "    \n",
    "    preds_dev, ner_loss_dev = model.test(loader_dev)  \n",
    "    \n",
    "    eval_metrics_crf(preds_dev, metrics, idner_dev[:test_lim], sents_dev_03[:test_lim], \n",
    "                     ner2idx, idx2ner, model_path, dev=True)\n",
    "    \n",
    "    val_loss_epoch = ner_loss_dev\n",
    "    \n",
    "    preds_test, ner_loss_test = model.test(loader_test)     \n",
    "    \n",
    "    eval_metrics_crf(preds_test, metrics, idner_test[:test_lim], sents_test_03[:test_lim], \n",
    "                         ner2idx, idx2ner, model_path)\n",
    "    \n",
    "    loss_epoch = np.mean(losses)\n",
    "    \n",
    "    print(\"Loss :  NER %s\" % (loss_epoch))\n",
    "    print(\"Dev loss : NER %s\" % (val_loss_epoch))\n",
    "    print(\"Test loss : NER %s\" % (ner_loss_test))\n",
    "    \n",
    "    torch.save(model.state_dict(), model_path + \"last_state_dict\")\n",
    "    if ner_loss_dev < best_ner: \n",
    "        print(\"New best score on dev.\")\n",
    "        print(\"Saving model...\")\n",
    "        torch.save(model.state_dict(), model_path + \"best_state_dict\")\n",
    "        \n",
    "        best_ner = ner_loss_dev\n",
    "    \n",
    "    metrics[\"ner\"][\"val_loss_dev\"].append(val_loss_epoch)\n",
    "    metrics[\"ner\"][\"val_loss_test\"].append(ner_loss_test)    \n",
    "    metrics[\"ner\"][\"loss\"].append(loss_epoch)    \n",
    "\n",
    "    # Save learning curve\n",
    "    save_plot(metrics, model_path)\n",
    "    with open(model_path + \"metrics.p\", \"wb\") as file:\n",
    "        pickle.dump(metrics, file)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
